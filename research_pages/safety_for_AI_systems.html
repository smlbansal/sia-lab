<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Somil Bansal</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../assets/css/main.css" />


		<script>
		  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

		  ga('create', 'UA-78683315-1', 'auto');
		  ga('send', 'pageview');

		</script>
	</head>
	<body class="is-preload">
		<style type="text/css">
			.tg  {border-collapse:collapse;border-spacing:0;}
			.tg td{border-color:black;border-style:none;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
				overflow:hidden;padding:10px 5px;word-break:normal;}
			.tg th{border-color:black;border-style:none;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
				font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
			.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}
			.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
		</style>
		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="../index.html" class="logo">Safety for AI Systems</a>
									<ul class="icons">
										<li><a href="https://github.com/sia-lab-git" class="icon brands fa-github"><span class="label">Medium</span></a></li>
									</ul>
								</header>

							<!-- Content -->
								<section>
									<p> Generative AI tools such as large language models (LLMs) and diffusion models have become ubiquitous in everyday life, raising urgent safety concerns due to their potential to generate harmful content. In this research thrust, we develop control-theoretic algorithms that monitor safety during inference and steer these models toward producing safer outputs without compromising their performance.</p>
									

									<!-- Content -->
									<h2 id="content"> Preemptive Detection and Steering of LLM Misalignment via Latent Reachability </h2>
									<div id="banner" class="content">
										<div class="image26 object">
											<img src="../images/paper_images/llm_safety_sathwik.png" style="height: auto; width: 550px" alt="">
										</div>
										<span class="content">
											<p>
												 Large language models (LLMs) are now ubiquitous in everyday tools, raising urgent safety concerns about their tendency to generate harmful content. The dominant safety approach-reinforcement learning from human feedback (RLHF) effectively shapes model behavior during training but offers no safeguards at inference time, where unsafe continuations may still arise. We propose BRT-ALIGN, a reachability-based framework that brings control-theoretic safety tools to LLM inference. BRT-ALIGN models autoregressive generation as a dynamical system in latent space and learn a safety value function via backward reachability, estimating the worst-case evolution of a trajectory. This enables two complementary mechanisms: (1) a runtime monitor that forecasts unsafe completions several tokens in advance, and (2) a least-restrictive steering filter that minimally perturbs latent states to redirect generation away from unsafe regions. Experiments across multiple LLMs and toxicity benchmarks demonstrate that BRT-ALIGN provides more accurate and earlier detection of unsafe continuations than baselines. Moreover, for LLM safety alignment, BRT-ALIGN substantially reduces unsafe generations while preserving sentence diversity and coherence. Qualitative results further highlight emergent alignment properties: BRT-ALIGN consistently produces responses that are less violent, less profane, less offensive, and less politically biased. Together, these findings demonstrate that reachability analysis provides a principled and practical foundation for inference-time LLM safety.
											</p>
											<a href="https://arxiv.org/pdf/2509.21528?">[Paper]</a> 
										</span>
									</div> 

									
									<h2>For a more exhaustive list of our research work please go <a href="../publications.html"><u>HERE!</u></a>  </h2> 
								</section>
						</div>
					</div>

													<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">

							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>SIA Lab</h2>
									</header>
									<ul>
										<li><a href="../index.html">Homepage</a></li>
										
										<li><a href="../research.html">Research</a></li>
										<li><a href="../people.html">People</a></li>
										<li><a href="../teaching.html">Teaching</a></li>
										<li><a href="../tutorial.html">Tutorials and Workshops</a></li>
										<!-- <li>
											<span class="opener">Outreach</span>
											<ul>
												<li><a href="../tutorial.html">Tutorials and Workshops</a></li>
												<li><a href="../ug_and_g.html">UG and Graduate</a></li>
												<li><a href="../k12.html">K-12</a></li>
											</ul>
										</li> -->
										<li><a href="../publications.html">Publications</a></li>
										<li><a href="../talks.html">Talks</a></li>
										<li><a href="../joinus.html">Join Us!</a></li>
									</ul>
								</nav>

						</div>
					</div>
            
                        </div>

			</div>

		<!-- Scripts -->
			<script src="../assets/js/jquery.min.js"></script>
			<script src="../assets/js/browser.min.js"></script>
			<script src="../assets/js/breakpoints.min.js"></script>
			<script src="../assets/js/util.js"></script>
			<script src="../assets/js/main.js"></script>

	</body>
</html>